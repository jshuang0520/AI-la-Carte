# TODO

- this is just a summary, if there are anythiing missed, please feel free to modify it

# together
- one page documentation (for our methodology)

# Johnson
- add one more question: which services do you need? (and a drop-down list)

(15 services,
 {'Behavioral Healthcare',
  'Case management',
  'Childcare',
  'ESL',
  'Financial advising',
  'Financial assistance',
  "Gov't benefits enrollment",
  'Healthcare',
  'Housing',
  "Info on gov't benefits",
  'Job training/ workforce development',
  'Legal services',
  'Non-food items',
  'Nutrition Education Materials and Resources',
  'Programming/ support for older adults'})

# Peeyush
- for filtering and sorting services: e.g. 
- What is the minimum value for the variable "distance" that a store is guaranteed to exist?
- flags to deal with (encode for our SQL-like query workflow)
    - as needed
    - until food runs out
    - do not filter it out, and preserve those records at the bottom of the recommended list
- (optional) after each filter search, we can return the log that shows how many stores are left after this search


# Krishna
- work on vector db with metadata filter e.g. Pinecone
- design how to store the documents in our workflow

---

# references

## When I do the LLM RAG search is it possible to ask the llm to skip certain (different) docs in each query?
You can try, but there's no built‑in “skip these docs” parameter. Instead, you would need to craft your queries with additional context or negative keywords to discourage matching those documents. In other words, while you can instruct the LLM (or the retrieval system) to focus on certain content and avoid others, it all depends on how well the underlying search algorithm interprets your query. So rather than an explicit exclusion flag, you’d have to phrase your query like:

"Answer this question using information from the relevant sources, but ignore content from [Doc A] and [Doc B]."

This approach may work if the retrieval system recognizes and applies the negative constraints. However, if you need more precise control, you might need to post-process the results or set up custom filtering rules in your retrieval-augmented generation workflow.


## how to store the documents into db for RAG search? is it like store documents into vector dbs, so that we need to embed our documents into like faiss?
Yes, that's exactly the idea. In a typical RAG setup, you take your documents and pass them through an embedding model to convert each document (or document chunk) into a dense vector. These vectors are then stored in a vector database—like FAISS, Pinecone, Weaviate, or Milvus—along with metadata such as document IDs, source information, or text snippets.

When a query comes in, you embed the query in the same way, then perform a similarity search (using metrics like cosine similarity) against the stored vectors to retrieve the most relevant documents. Those retrieved documents are then used to augment the context for your LLM, improving the quality and specificity of the answer.

So yes, embedding your documents into a vector space and storing them in a vector database (such as FAISS) is a common approach for RAG search.

## so we have a concern that since documents are embedded how to ensure that the document id we want to skip is the exact document we want to skip?
When documents are embedded, it's crucial to store additional metadata—like a unique document ID—alongside each embedding. This way, even though the retrieval is based on vector similarity, you still have an exact identifier for each document. Here are a couple of approaches to ensure you skip the correct document:

• In many vector databases (e.g., Pinecone, Weaviate, Milvus), you can attach metadata fields (such as "doc_id") to each embedding. When performing a search, you can include a filter that explicitly excludes any document with a specific ID.

• If you're using a solution like FAISS, which doesn't natively support metadata filtering, you can perform the similarity search first and then filter out any results with the unwanted document ID during post-processing.

This metadata-driven approach guarantees that even if two documents have similar embeddings, the system can precisely identify and skip the one you want to exclude.

## aside faiss, what kind of the vector dbs allow us to store metadata for further filtering? e.g. metadata like document id
Yes, several vector databases support storing and filtering by metadata. For example:

• Pinecone is a managed vector DB that lets you attach metadata (like document IDs) to each vector. You can then use filtering during searches to exclude or include documents based on those metadata fields.

• Weaviate is an open-source vector database that allows you to store custom metadata with your embeddings. It provides built-in filtering capabilities to query based on metadata such as document IDs, tags, or other attributes.

• Milvus (in its more recent versions) supports metadata filtering alongside vector similarity searches, allowing you to include or exclude documents based on specific metadata criteria.

• Qdrant is another vector search engine that enables rich metadata storage and filtering, making it easy to precisely control which documents appear in your search results.

These databases let you embed your documents and then attach metadata (like document IDs), ensuring that you can filter out exact documents you want to skip during the retrieval process.

## can you provide a sample code to use python to store documents into Pinecone?
sample document looks like:
store: hi-life
store_id: ab123
serving people: asian
opening hours: Mon - Fri 09:00 to 18:00
address: Vassar Drive, college park, maryland
distance: 10 miles from your home


store: 7-11
store_id: ab456
serving people: American
opening hours: Mon - Fri 09:00 to 18:00
address: Minquan E Rd, Taipei
distance: 50 miles from your home

and my query is to find chinese food using RAG, searching the documents mentioned above, please provide me the sample code / query

> Below is an example Python script that demonstrates how to:

• Initialize a Pinecone index
• Upsert a couple of sample documents (with metadata) into Pinecone
• Query the index for “Chinese food” using a filter (which in this example filters by a metadata field, e.g., serving_people equals "Chinese")

For simplicity, a dummy embedding function is used (in practice you’d use a real embedding model such as OpenAI’s or SentenceTransformers). Make sure you replace the placeholders for API key and environment with your own.

```python
import pinecone
import numpy as np

# Initialize Pinecone (replace with your actual API key and environment)
pinecone.init(api_key="YOUR_API_KEY", environment="YOUR_ENVIRONMENT")

index_name = "store-index"
vector_dim = 768  # This dimension should match the output of your embedding model

# Create the index if it doesn't exist
if index_name not in pinecone.list_indexes():
    pinecone.create_index(index_name, dimension=vector_dim)

# Connect to the index
index = pinecone.Index(index_name)

# Dummy embedding function for demonstration
def dummy_embedding(text):
    # In a real-world scenario, you would use a proper embedding model.
    # Here we generate a random vector of the desired dimension.
    return np.random.rand(vector_dim).tolist()

# Sample documents to upsert
documents = [
    {
        "id": "ab123",
        "store": "hi-life",
        "serving_people": "asian",
        "opening_hours": "Mon - Fri 09:00 to 18:00",
        "address": "Vassar Drive, College Park, Maryland",
        "distance": "10 miles from your home"
    },
    {
        "id": "ab456",
        "store": "7-11",
        "serving_people": "American",
        "opening_hours": "Mon - Fri 09:00 to 18:00",
        "address": "Minquan E Rd, Taipei",
        "distance": "50 miles from your home"
    }
]

# Upsert documents into Pinecone
upsert_data = []
for doc in documents:
    # Combine text fields for the embedding. You may choose the best combination for your use case.
    text = f"{doc['store']} {doc['serving_people']} {doc['opening_hours']} {doc['address']} {doc['distance']}"
    embedding = dummy_embedding(text)
    
    # Prepare the vector along with its metadata
    vector = (doc["id"], embedding, {
        "store": doc["store"],
        "serving_people": doc["serving_people"],
        "opening_hours": doc["opening_hours"],
        "address": doc["address"],
        "distance": doc["distance"]
    })
    upsert_data.append(vector)

# Upsert the vectors into the index
index.upsert(vectors=upsert_data)
print("Documents upserted into Pinecone.")

# --- Querying ---

# Suppose you want to search for Chinese food. In this example, we want to find documents where the serving_people metadata equals "Chinese".
# Note: Our sample documents do not have Chinese food, so this query will return no results.
query_text = "Chinese food"
query_embedding = dummy_embedding(query_text)

# Build a filter to include only documents serving "Chinese" people
query_filter = {"serving_people": {"$eq": "Chinese"}}

# Query the index with the filter
result = index.query(vector=query_embedding, filter=query_filter, top_k=5, include_metadata=True)

print("Query results:")
print(result)
```

Explanation
Initialization & Index Creation:
The code initializes Pinecone and creates (if necessary) an index named store-index with a vector dimension matching the embedding model's output.

Embedding & Upserting Documents:

A dummy embedding function produces random vectors.

Sample documents (with fields like store name, serving_people, etc.) are embedded and upserted into Pinecone along with metadata (e.g., store, serving_people).

Querying:

A query for “Chinese food” is embedded.

A filter is applied so that only documents with serving_people equal to "Chinese" are returned.

In this example, since neither document serves "Chinese," no results will be returned. In your real use case, adjust the metadata and filter to suit your requirements.

Remember to replace "YOUR_API_KEY" and "YOUR_ENVIRONMENT" with your actual Pinecone credentials and adjust the embedding logic as needed for your application.